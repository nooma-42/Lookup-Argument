#!/bin/bash

# Benchmark with Incremental CSV Writing and Failure Tracking
# This script demonstrates the new features added to the benchmark system

echo "üöÄ Lookup Argument Benchmark with Incremental CSV Writing"
echo "========================================================"

# Clean up previous results
if [ -f "benchmark_results.csv" ]; then
    echo "üóëÔ∏è  Removing previous benchmark_results.csv"
    rm benchmark_results.csv
fi

if [ -f "benchmark_failures.csv" ]; then
    echo "üóëÔ∏è  Removing previous benchmark_failures.csv"
    rm benchmark_failures.csv
fi

echo ""
echo "üìã Running comprehensive benchmark suite..."
echo "   ‚Ä¢ Systems: ALL (CQ, Baloo, LogupGKR, Plookup, Caulk, Lasso)"
echo "   ‚Ä¢ K values: 5-12 (table sizes from 32 to 4096)"
echo "   ‚Ä¢ N:n ratios: 2, 4, 8, 16"
echo "   ‚Ä¢ Total tasks: ~192 (6 systems √ó 8 k-values √ó 4 ratios)"
echo "   ‚Ä¢ Output format: Table"
echo ""
echo "‚ö†Ô∏è  Warning: This is a comprehensive benchmark that may take several hours!"
echo "    Results will be saved incrementally, so you can monitor progress."
echo ""

# Ask for confirmation
read -p "Do you want to proceed with this comprehensive benchmark? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "‚ùå Benchmark cancelled."
    exit 1
fi

echo "üöÄ Starting comprehensive benchmark..."
echo "üíæ Results will be saved to benchmark_results.csv as each test completes"
echo ""

# Run the benchmark
cd benchmark
time cargo bench --bench proof_system -- \
    --system all \
    --k 5..14 \
    --ratio 2,4,8,16 \
    --format table

echo ""
echo "üìä Benchmark completed! Checking results..."

# Check if files exist and show their content
if [ -f "benchmark_results.csv" ]; then
    echo ""
    echo "‚úÖ benchmark_results.csv created successfully!"
    echo "üìè File size: $(wc -c < benchmark_results.csv) bytes"
    echo "üìä Number of results: $(($(wc -l < benchmark_results.csv) - 1))"
    echo ""
    echo "üìñ First few lines:"
    head -5 benchmark_results.csv
    echo ""
    echo "üìñ Last few lines:"
    tail -3 benchmark_results.csv
    echo ""
    echo "üìà Results by system:"
    tail -n +2 benchmark_results.csv | cut -d',' -f1 | sort | uniq -c | sort -nr
else
    echo "‚ùå benchmark_results.csv not found!"
fi

if [ -f "benchmark_failures.csv" ]; then
    if [ -s "benchmark_failures.csv" ]; then
        echo ""
        echo "‚ö†Ô∏è  benchmark_failures.csv contains failures:"
        echo "üìè File size: $(wc -c < benchmark_failures.csv) bytes"
        echo "‚ùå Number of failures: $(($(wc -l < benchmark_failures.csv) - 1))"
        echo ""
        echo "üìñ Failure summary by system:"
        tail -n +2 benchmark_failures.csv | cut -d',' -f1 | sort | uniq -c | sort -nr
        echo ""
        echo "üìñ First few failure details:"
        head -5 benchmark_failures.csv
    else
        echo ""
        echo "‚úÖ benchmark_failures.csv exists but is empty (no failures)"
    fi
else
    echo "‚ùå benchmark_failures.csv not found!"
fi

echo ""
echo "üéØ Comprehensive Benchmark Features Demonstrated:"
echo "   ‚úÖ All proof systems tested - CQ, Baloo, LogupGKR, Plookup, Caulk, Lasso"
echo "   ‚úÖ Wide parameter range - k=5..12, ratios=2,4,8,16"
echo "   ‚úÖ Incremental CSV writing - results saved as each benchmark completes"
echo "   ‚úÖ Failure tracking - any failed benchmarks are logged separately"
echo "   ‚úÖ Parallel execution - multiple benchmarks run simultaneously"
echo "   ‚úÖ Progress tracking - real-time progress updates with percentages"
echo "   ‚úÖ Comprehensive reporting - separate files for successes and failures"
echo ""
echo "üí° Benefits:"
echo "   üìÅ Partial results preserved even if the process is interrupted"
echo "   üîç Easy identification of which specific configurations failed"
echo "   ‚ö° Faster execution through parallelization"
echo "   üìä Machine-readable CSV format for further analysis"
echo "   üéØ Complete coverage of all proof systems and parameters"
echo "   üìà Automatic plot generation for all key performance metrics"
echo ""
echo "üóÇÔ∏è  Output files:"
echo "   üìä benchmark_results.csv - All successful benchmark results"
echo "   ‚ùå benchmark_failures.csv - Any failed benchmark attempts"
echo "   üé® plots/*.png - Automatically generated performance visualization plots"
echo ""

# Generate a quick analysis if we have results
if [ -f "benchmark_results.csv" ] && [ -s "benchmark_results.csv" ]; then
    echo "üìä Quick Analysis:"
    echo "   ‚Ä¢ Total successful benchmarks: $(($(wc -l < benchmark_results.csv) - 1))"
    
    # Find fastest and slowest setups
    if [ $(wc -l < benchmark_results.csv) -gt 1 ]; then
        echo "   ‚Ä¢ Fastest setup time: $(tail -n +2 benchmark_results.csv | cut -d',' -f4 | sort -n | head -1)ms"
        echo "   ‚Ä¢ Slowest setup time: $(tail -n +2 benchmark_results.csv | cut -d',' -f4 | sort -n | tail -1)ms"
        echo "   ‚Ä¢ Fastest prove time: $(tail -n +2 benchmark_results.csv | cut -d',' -f5 | sort -n | head -1)ms"
        echo "   ‚Ä¢ Slowest prove time: $(tail -n +2 benchmark_results.csv | cut -d',' -f5 | sort -n | tail -1)ms"
    fi
fi

echo ""
echo "Done! üéâ"
# Generate plots automatically if benchmarks completed successfully
if [ -f "benchmark_results.csv" ] && [ -s "benchmark_results.csv" ] && [ $(wc -l < "benchmark_results.csv") -gt 1 ]; then
    echo "üìä Generating performance plots..."
    echo ""
    
    # Check if Python is available
    if command -v python3 &> /dev/null; then
        PYTHON_CMD="python3"
    elif command -v python &> /dev/null; then
        PYTHON_CMD="python"
    else
        echo "‚ö†Ô∏è  Python not found. Skipping plot generation."
        echo "   You can manually generate plots later using: python diagram.py"
        PYTHON_CMD=""
    fi
    
    if [ -n "$PYTHON_CMD" ]; then
        # Create plots directory
        mkdir -p plots
        
        echo "üé® Generating comprehensive performance plots..."
        echo "   üìä Setup Time plots..."
        $PYTHON_CMD diagram.py --metrics SetupTime_ms --output-dir plots/ --protocols Lasso,LogupGKR,CQ,Plookup,Baloo,Caulk
        
        echo "   üîç Verify Time plots..."
        $PYTHON_CMD diagram.py --metrics VerifyTime_ms --output-dir plots/ --protocols Lasso,LogupGKR,CQ,Plookup,Baloo,Caulk
        
        echo "   ‚è±Ô∏è  Total Time plots..."
        $PYTHON_CMD diagram.py --metrics TotalTime_ms --output-dir plots/ --protocols Lasso,LogupGKR,CQ,Plookup,Baloo,Caulk
        
        echo "   üì¶ Proof Size plots..."
        $PYTHON_CMD diagram.py --metrics ProofSize_bytes --output-dir plots/ --protocols Lasso,LogupGKR,CQ,Plookup,Baloo,Caulk
        
        echo "   üöÄ Proving Time plots (original)..."
        $PYTHON_CMD diagram.py --metrics ProveTime_ms --output-dir plots/ --protocols Lasso,LogupGKR,CQ,Plookup,Baloo,Caulk
        
        echo ""
        echo "üìà Plot generation completed!"
        echo ""
        
        # Show generated plots summary
        if [ -d "plots" ]; then
            PLOT_COUNT=$(ls -1 plots/*.png 2>/dev/null | wc -l)
            if [ $PLOT_COUNT -gt 0 ]; then
                echo "üéØ Generated $PLOT_COUNT performance plots in plots/ directory:"
                echo ""
                echo "üìä Setup Time plots:"
                ls -1 plots/setuptimems_*.png 2>/dev/null | sed 's/^/   ‚Ä¢ /'
                echo ""
                echo "üîç Verify Time plots:"
                ls -1 plots/verifytimems_*.png 2>/dev/null | sed 's/^/   ‚Ä¢ /'
                echo ""
                echo "‚è±Ô∏è  Total Time plots:"
                ls -1 plots/totaltimems_*.png 2>/dev/null | sed 's/^/   ‚Ä¢ /'
                echo ""
                echo "üì¶ Proof Size plots:"
                ls -1 plots/proofsizebytes_*.png 2>/dev/null | sed 's/^/   ‚Ä¢ /'
                echo ""
                echo "üöÄ Proving Time plots:"
                ls -1 plots/provetimems_*.png 2>/dev/null | sed 's/^/   ‚Ä¢ /'
                echo ""
            else
                echo "‚ö†Ô∏è  No plots were generated (check for errors above)"
            fi
        fi
        
        echo "üí° Plot Usage:"
        echo "   ‚Ä¢ SetupTime: Shows preprocessing/setup performance across systems"
        echo "   ‚Ä¢ VerifyTime: Shows verification performance (important for deployment)"
        echo "   ‚Ä¢ TotalTime: Shows end-to-end performance including all phases"
        echo "   ‚Ä¢ ProofSize: Shows proof size comparison (important for storage/bandwidth)"
        echo "   ‚Ä¢ ProveTime: Shows core proving time performance"
        echo ""
        echo "üé® Custom Plot Generation:"
        echo "   ‚Ä¢ All metrics for single ratio: python diagram.py --metrics SetupTime_ms,VerifyTime_ms,TotalTime_ms,ProofSize_bytes --single-ratio 4"
        echo "   ‚Ä¢ Specific protocols only: python diagram.py --metrics TotalTime_ms --protocols Lasso,LogupGKR"
        echo "   ‚Ä¢ Different output directory: python diagram.py --output-dir custom_plots/"
        echo ""
    fi
else
    echo "‚ö†Ô∏è  No benchmark results available for plot generation."
fi

echo ""
echo "üí° Next steps:"
echo "   ‚Ä¢ Review generated plots in plots/ directory for performance insights"
echo "   ‚Ä¢ Analyze benchmark_results.csv for detailed numerical data"
echo "   ‚Ä¢ Check benchmark_failures.csv for any issues that need attention"
echo "   ‚Ä¢ Use the CSV data and plots for research or optimization decisions" 